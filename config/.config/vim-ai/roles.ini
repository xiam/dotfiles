# local models

[phi]
options.endpoint_url = https://litellm.internal.xiam.dev/ollama/v1/chat/completions
options.model = phi4-reasoning:latest

[deepseek]
options.endpoint_url = https://litellm.internal.xiam.dev/ollama/v1/chat/completions
options.model = deepseek-coder:33b-instruct-q3_K_M

[llama]
options.endpoint_url = https://litellm.internal.xiam.dev/ollama/v1/chat/completions
options.model = codellama:34b-code-q4_0

[qwen]
options.endpoint_url = https://litellm.internal.xiam.dev/ollama/v1/chat/completions
options.model = qwen2.5-coder:32b
#options.model = qwen2.5-coder:14b

[codestral]
options.endpoint_url = https://litellm.internal.xiam.dev/ollama/v1/chat/completions
options.model = codestral:22b-v0.1-q4_K_M

# commercial models

[gemini]
options.model = gemini/gemini-2.5-pro

[gemini2]
options.model = gemini/gemini-2.5-pro

[gemini3]
options.model = gemini/gemini-3-pro-preview

[gemini-flash]
options.model = gemini/gemini-2.5-flash

[claude]
options.model = anthropic/claude-sonnet-4-5-20250929

[claude-opus]
options.model = anthropic/claude-opus-4-1-20250805

[claude-haiku]
options.model = anthropic/claude-haiku-4-5-20251001

[gpt5]
options.model = openai/gpt-5-2025-08-07

[gpt5-mini]
options.model = openai/gpt-5-mini-2025-08-07
