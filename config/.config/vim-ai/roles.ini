# local models

[phi]
options.endpoint_url = https://litellm.local.xiam.dev/ollama/v1/chat/completions
options.model = phi4-reasoning:14b

[deepseek]
options.endpoint_url = https://litellm.local.xiam.dev/ollama/v1/chat/completions
options.model = deepseek-coder-v2:16b

[llama]
options.endpoint_url = https://litellm.local.xiam.dev/ollama/v1/chat/completions
options.model = codellama:34b

[qwen]
options.endpoint_url = https://litellm.local.xiam.dev/ollama/v1/chat/completions
options.model = qwen2.5-coder:14b

[codestral]
options.endpoint_url = https://litellm.local.xiam.dev/ollama/v1/chat/completions
options.model = codestral:22b

# commercial models

[gemini]
options.model = gemini/gemini-2.5-pro

[flash]
options.model = gemini/gemini-2.5-flash

[claude]
options.model = anthropic/claude-opus-4-1-20250805
options.request_timeout = 120

[openai]
options.model = openai/gpt-5-2025-08-07
